{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCgA7MOA-_hU"
      },
      "source": [
        "# Fine-tuning LLMs\n",
        "\n",
        "In this section, we demonstrate how to fine-tune LLMs. Note that you will need to use a GPU for this section. You can do so by clicking \"Runtime -> Change runtime type\" and selecting a GPU.\n",
        "\n",
        "Let's load all the necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3h39QrjT-_hW",
        "outputId": "5cf5f133-e0da-488e-aa75-fe9166c6e6e7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/daniel/mlops/llmops/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# importing libraries\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import transformers\n",
        "from datasets import Features, Value, Dataset, DatasetDict\n",
        "import comet_ml\n",
        "import comet_llm\n",
        "import os\n",
        "import numpy as np\n",
        "import pickle\n",
        "import json\n",
        "import pandas as pd\n",
        "import torch\n",
        "from comet_ml import API\n",
        "from comet_ml import ExistingExperiment\n",
        "\n",
        "transformers.set_seed(35)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# setting comet api and workspace\n",
        "\n",
        "api = API(api_key=os.environ[\"COMET_API_KEY\"])\n",
        "COMET_WORKSPACE = os.environ[\"COMET_WORKSPACE\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxyKxm_i-_hX"
      },
      "source": [
        "### Dataset Preparation\n",
        "\n",
        "The code below loads the datasets and converts them into the proper format. We are also sampling the dataset. You can choose different sample sizes to run different experiments. More samples typically lead to a better performing model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "q4fQAVrW-_hX"
      },
      "outputs": [],
      "source": [
        "# loads the data from the jsonl files\n",
        "emotion_dataset_train = pd.read_json(path_or_buf=\"https://raw.githubusercontent.com/comet-ml/comet-llmops/main/data/merged_training_sample_prepared_train.jsonl\", lines=True)\n",
        "emotion_dataset_val_temp = pd.read_json(path_or_buf=\"https://raw.githubusercontent.com/comet-ml/comet-llmops/main/data/merged_training_sample_prepared_valid.jsonl\", lines=True)\n",
        "\n",
        "# takes first half of samples from emotion_dataset_val_temp and make emotion_dataset_val\n",
        "emotion_dataset_val = emotion_dataset_val_temp.iloc[:int(len(emotion_dataset_val_temp)/2)]\n",
        "\n",
        "# takes second half of samples from emotion_dataset_val_temp and make emotion_dataset_test\n",
        "emotion_dataset_test = emotion_dataset_val_temp.iloc[int(len(emotion_dataset_val_temp)/2):]\n",
        "\n",
        "sample = True\n",
        "\n",
        "if sample == True:\n",
        "    final_ds = DatasetDict({\n",
        "        \"train\": Dataset.from_pandas(emotion_dataset_train.sample(50)),\n",
        "        \"validation\": Dataset.from_pandas(emotion_dataset_val.sample(50)),\n",
        "        \"test\": Dataset.from_pandas(emotion_dataset_test.sample(50))\n",
        "    })\n",
        "else:\n",
        "    final_ds = DatasetDict({\n",
        "        \"train\": Dataset.from_pandas(emotion_dataset_train),\n",
        "        \"validation\": Dataset.from_pandas(emotion_dataset_val),\n",
        "        \"test\": Dataset.from_pandas(emotion_dataset_test)\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['prompt', 'completion', '__index_level_0__'],\n",
              "        num_rows: 50\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['prompt', 'completion', '__index_level_0__'],\n",
              "        num_rows: 50\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['prompt', 'completion', '__index_level_0__'],\n",
              "        num_rows: 50\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOeGTdbn-_hY"
      },
      "source": [
        "### Tokenize Dataset\n",
        "\n",
        "The code below defines a tokenizer and uses the Hugging Face tokenizer to tokenize the datasets. This is the format the model expects so this is an important step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3hQeg3x4-_hY",
        "outputId": "a80bce3f-aa36-4994-ee64-8573cc3a742d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading (…)okenizer_config.json: 100%|██████████| 2.54k/2.54k [00:00<00:00, 4.50MB/s]\n",
            "Downloading spiece.model: 100%|██████████| 792k/792k [00:01<00:00, 685kB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<00:00, 5.69MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100%|██████████| 2.42M/2.42M [00:00<00:00, 7.86MB/s]\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 2978.32 examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 3037.63 examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 3058.50 examples/s]\n"
          ]
        }
      ],
      "source": [
        "# model checkpoint\n",
        "model_checkpoint = \"google/flan-t5-base\"\n",
        "\n",
        "# We'll create a tokenizer from model checkpoint\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=False)\n",
        "\n",
        "# We'll need padding to have same length sequences in a batch\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# prefix\n",
        "prefix_instruction = \"Classify the provided piece of text into one of the following emotion labels.\\n\\nEmotion \\\n",
        "    labels: ['anger', 'fear', 'joy', 'love', 'sadness', 'surprise']\"\n",
        "\n",
        "# Define a tokenization function that first concatenates text and target\n",
        "def tokenize_function(example):\n",
        "    merged = prefix_instruction + \"\\n\\n\" + \"Text: \" + example[\"prompt\"].strip(\"\\n\\n###\\n\\n\") + \"\\n\\n\" + \"\\\n",
        "        Emotion output:\" + example[\"completion\"].strip(\" \").strip(\"\\n\")\n",
        "    batch = tokenizer(merged, padding='max_length', truncation=True)\n",
        "    batch[\"labels\"] = batch[\"input_ids\"].copy()\n",
        "    return batch\n",
        "\n",
        "# Apply it on our dataset, and remove the text columns\n",
        "tokenized_datasets = final_ds.map(tokenize_function, remove_columns=[\"prompt\", \"completion\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 50\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 50\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 50\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_datasets "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KEKhD84-_hY"
      },
      "source": [
        "### Finetuning Model\n",
        "\n",
        "Once the datasets have been tokenized, it's time to finetune the model. We are using the HF Trainer to simplify the finetuning code. In the code below, it's also important to initialize a Comet project which allows tracking the experimental results to Comet. You can also set the `COMET_LOG_ASSETS` to `True` to store all artifacts to Comet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4jK612v-_hZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1;38;5;196mCOMET ERROR:\u001b[0m Heartbeat processing error\n"
          ]
        }
      ],
      "source": [
        "# initialize comet_ml\n",
        "comet_ml.init(project_name=\"emotion-classification\")\n",
        "\n",
        "# training an autoregressive language model from a pretrained checkpoint\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)\n",
        "\n",
        "# set this to log HF results and assets to Comet\n",
        "os.environ[\"COMET_LOG_ASSETS\"] = \"True\"\n",
        "\n",
        "# HF Trainer\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    num_train_epochs=1,\n",
        "    output_dir=\"./results\",\n",
        "    overwrite_output_dir=True,\n",
        "    logging_steps=1,\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=1e-4,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=5,\n",
        "    save_steps=7,\n",
        "    auto_find_batch_size=True\n",
        ")\n",
        "\n",
        "# instantiate HF Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# run trainer\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kc7lwASN-_hZ"
      },
      "source": [
        "The code below stores the results locally:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJbcigg7-_hZ"
      },
      "outputs": [],
      "source": [
        "# save the model\n",
        "trainer.save_model(\"./results\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sl9KVZUI-_hZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tDTdiQ1-_hZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDJ-aCyf-_ha"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQX_7B6v-_ha"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeCyvB17-_ha"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af_UvgoQ-_ha"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_o_asQU-_ha"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAn42Etg-_ha"
      },
      "source": [
        "### Register Model\n",
        "\n",
        "The code below registers the model to Comet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5ycM0ZL-_ha"
      },
      "outputs": [],
      "source": [
        "# set existing experiment\n",
        "\n",
        "\n",
        "COMET_API_KEY = \"COMET_API_KEY\"\n",
        "\n",
        "experiment = ExistingExperiment(api_key=os.environ[\"COMET_API_KEY\"], previous_experiment=\"097ab78e6e154f24b8090a1a7dd6abb8\")\n",
        "experiment.log_model(\"Emotion-T5-Base\", \"results/checkpoint-7\")\n",
        "experiment.register_model(\"Emotion-T5-Base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l46sSt7I-_hb"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHWoykLr-_hb"
      },
      "source": [
        "### Deploy Model\n",
        "\n",
        "The code below helps to download the model and specific version to whatever environment you are deploying from."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fn0IoLmh-_hb"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# model name\n",
        "model_name = \"emotion-flan-t5-base\"\n",
        "\n",
        "#get the Model object\n",
        "model = api.get_model(workspace=COMET_WORKSPACE, model_name=model_name)\n",
        "\n",
        "# Download a Registry Model:\n",
        "model.download(\"1.0.0\", \"./deploy\", expand=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "comet",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
